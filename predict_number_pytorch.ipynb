{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-05T18:22:59.855641400Z",
     "start_time": "2024-02-05T18:22:58.361640600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[ 0.],\n",
      "         [ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.],\n",
      "         [ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.],\n",
      "         [11.],\n",
      "         [12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.],\n",
      "         [16.],\n",
      "         [17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.],\n",
      "         [21.],\n",
      "         [22.],\n",
      "         [23.],\n",
      "         [24.],\n",
      "         [25.],\n",
      "         [26.],\n",
      "         [27.],\n",
      "         [28.],\n",
      "         [29.]]])\n",
      "y: tensor([[[36.],\n",
      "         [37.],\n",
      "         [38.],\n",
      "         [39.],\n",
      "         [40.],\n",
      "         [41.],\n",
      "         [42.],\n",
      "         [43.],\n",
      "         [44.],\n",
      "         [45.],\n",
      "         [46.],\n",
      "         [47.],\n",
      "         [48.],\n",
      "         [49.],\n",
      "         [50.],\n",
      "         [51.],\n",
      "         [52.],\n",
      "         [53.],\n",
      "         [54.],\n",
      "         [55.],\n",
      "         [56.],\n",
      "         [57.],\n",
      "         [58.],\n",
      "         [59.],\n",
      "         [60.],\n",
      "         [61.],\n",
      "         [62.],\n",
      "         [63.],\n",
      "         [64.],\n",
      "         [65.]]])\n",
      "Loss: 2365.072998046875\n",
      "Loss: 46436.37109375\n",
      "Loss: 103939.75\n",
      "Loss: 25365.439453125\n",
      "Loss: 2251.610595703125\n",
      "Loss: 389.458984375\n",
      "Loss: 476.8306579589844\n",
      "Loss: 1378.070556640625\n",
      "Loss: 3029.54736328125\n",
      "Loss: 3310.54833984375\n",
      "Loss: 1369.734130859375\n",
      "Loss: 360.9452209472656\n",
      "Loss: 3220.131103515625\n",
      "Loss: 3968.537353515625\n",
      "Loss: 1002.9874877929688\n",
      "Loss: 489.98876953125\n",
      "Loss: 2520.872802734375\n",
      "Loss: 1789.688232421875\n",
      "Loss: 70.35861206054688\n",
      "Loss: 1458.4163818359375\n",
      "Loss: 2188.945556640625\n",
      "Loss: 337.67138671875\n",
      "Loss: 503.6897277832031\n",
      "Loss: 1598.656005859375\n",
      "Loss: 557.6939086914062\n",
      "Loss: 135.71499633789062\n",
      "Loss: 1189.94775390625\n",
      "Loss: 635.4817504882812\n",
      "Loss: 22.73257827758789\n",
      "Loss: 743.4371948242188\n",
      "Loss: 602.9921264648438\n",
      "Loss: 2.0224266052246094\n",
      "Loss: 446.27313232421875\n",
      "Loss: 529.535888671875\n",
      "Loss: 22.158817291259766\n",
      "Loss: 240.39805603027344\n",
      "Loss: 423.5417175292969\n",
      "Loss: 67.6712875366211\n",
      "Loss: 98.13491821289062\n",
      "Loss: 320.3267517089844\n",
      "Loss: 96.91939544677734\n",
      "Loss: 30.583812713623047\n",
      "Loss: 214.47259521484375\n",
      "Loss: 122.68983459472656\n",
      "Loss: 1.2148725986480713\n",
      "Loss: 125.18736267089844\n",
      "Loss: 122.5808334350586\n",
      "Loss: 3.579580545425415\n",
      "Loss: 60.80790328979492\n",
      "Loss: 105.76831817626953\n",
      "Loss: 19.538862228393555\n",
      "Loss: 19.519020080566406\n",
      "Loss: 78.24547576904297\n",
      "Loss: 31.989946365356445\n",
      "Loss: 3.2404251098632812\n",
      "Loss: 48.97991180419922\n",
      "Loss: 38.374759674072266\n",
      "Loss: 0.5601713061332703\n",
      "Loss: 25.184297561645508\n",
      "Loss: 35.754173278808594\n",
      "Loss: 4.2283406257629395\n",
      "Loss: 10.497265815734863\n",
      "Loss: 28.167268753051758\n",
      "Loss: 8.866678237915039\n",
      "Loss: 2.809580087661743\n",
      "Loss: 19.376895904541016\n",
      "Loss: 10.968424797058105\n",
      "Loss: 0.43951553106307983\n",
      "Loss: 11.837058067321777\n",
      "Loss: 10.984485626220703\n",
      "Loss: 0.3779584467411041\n",
      "Loss: 6.431471347808838\n",
      "Loss: 9.439091682434082\n",
      "Loss: 0.9915450215339661\n",
      "Loss: 3.207810401916504\n",
      "Loss: 7.374972343444824\n",
      "Loss: 1.5732485055923462\n",
      "Loss: 1.3997701406478882\n",
      "Loss: 5.423779010772705\n",
      "Loss: 1.7679190635681152\n",
      "Loss: 0.5712083578109741\n",
      "Loss: 3.8409857749938965\n",
      "Loss: 1.7313292026519775\n",
      "Loss: 0.1979730874300003\n",
      "Loss: 2.6831140518188477\n",
      "Loss: 1.5104148387908936\n",
      "Loss: 0.07037566602230072\n",
      "Loss: 1.8804187774658203\n",
      "Loss: 1.243770718574524\n",
      "Loss: 0.02735079452395439\n",
      "Loss: 1.3371907472610474\n",
      "Loss: 0.96938157081604\n",
      "Loss: 0.018639933317899704\n",
      "Loss: 0.9735031127929688\n",
      "Loss: 0.728393018245697\n",
      "Loss: 0.016125360503792763\n",
      "Loss: 0.7243624329566956\n",
      "Loss: 0.5242863893508911\n",
      "Loss: 0.016652580350637436\n",
      "Loss: 0.5502294898033142\n",
      "Loss: 0.36213091015815735\n",
      "Loss: 0.01867721788585186\n",
      "Loss: 0.42318597435951233\n",
      "Loss: 0.2359001338481903\n",
      "Loss: 0.02459850162267685\n",
      "Loss: 0.3268171548843384\n",
      "Loss: 0.14308960735797882\n",
      "Loss: 0.033030204474925995\n",
      "Loss: 0.2504805624485016\n",
      "Loss: 0.07737387716770172\n",
      "Loss: 0.04354676231741905\n",
      "Loss: 0.18751278519630432\n",
      "Loss: 0.035159461200237274\n",
      "Loss: 0.0528765507042408\n",
      "Loss: 0.13456577062606812\n",
      "Loss: 0.011225163005292416\n",
      "Loss: 0.05913173034787178\n",
      "Loss: 0.09007582068443298\n",
      "Loss: 0.001560554257594049\n",
      "Loss: 0.060031626373529434\n",
      "Loss: 0.05413072556257248\n",
      "Loss: 0.001515301177278161\n",
      "Loss: 0.05508933216333389\n",
      "Loss: 0.02738020196557045\n",
      "Loss: 0.006678001489490271\n",
      "Loss: 0.045018017292022705\n",
      "Loss: 0.010114090517163277\n",
      "Loss: 0.012996921315789223\n",
      "Loss: 0.03194313868880272\n",
      "Loss: 0.001742527005262673\n",
      "Loss: 0.01722210831940174\n",
      "Loss: 0.018757689744234085\n",
      "Loss: 0.00024945169570855796\n",
      "Loss: 0.01770200952887535\n",
      "Loss: 0.008200976066291332\n",
      "Loss: 0.002651386894285679\n",
      "Loss: 0.01451028324663639\n",
      "Loss: 0.0019366019405424595\n",
      "Loss: 0.005772849544882774\n",
      "Loss: 0.009243326261639595\n",
      "Loss: 1.5529994925600477e-05\n",
      "Loss: 0.0072791394777596\n",
      "Loss: 0.004128193948417902\n",
      "Loss: 0.0009701333474367857\n",
      "Loss: 0.006420435383915901\n",
      "Loss: 0.0009098649024963379\n",
      "Loss: 0.002669925568625331\n",
      "Loss: 0.003986867144703865\n",
      "Loss: 4.853504287893884e-05\n",
      "Loss: 0.0034362792503088713\n",
      "Loss: 0.0015092968242242932\n",
      "Loss: 0.0007202097913250327\n",
      "Loss: 0.0027932533994317055\n",
      "Loss: 0.00016879783652257174\n",
      "Loss: 0.0015654086600989103\n",
      "Loss: 0.0014060013927519321\n",
      "Loss: 0.00012284147669561207\n",
      "Loss: 0.001666498719714582\n",
      "Loss: 0.0003006630577147007\n",
      "Loss: 0.0006537014269270003\n",
      "Loss: 0.0010341017041355371\n",
      "Loss: 5.4127694966155104e-06\n",
      "Loss: 0.0009301785030402243\n",
      "Loss: 0.0003017306444235146\n",
      "Loss: 0.0002744713856372982\n",
      "Loss: 0.0006816143286414444\n",
      "Loss: 3.7205002172413515e-06\n",
      "Loss: 0.0005182608729228377\n",
      "Loss: 0.00023474633053410798\n",
      "Loss: 0.00012851579231210053\n",
      "Loss: 0.00042583569302223623\n",
      "Loss: 9.021422556543257e-06\n",
      "Loss: 0.0002984197053592652\n",
      "Loss: 0.00015493792307097465\n",
      "Loss: 7.073280721670017e-05\n",
      "Loss: 0.000256748782703653\n",
      "Loss: 5.819742000312544e-06\n",
      "Loss: 0.00017886138812173158\n",
      "Loss: 8.874433115124702e-05\n",
      "Loss: 4.55984118161723e-05\n",
      "Loss: 0.00015002508007455617\n",
      "Loss: 1.190632360703603e-06\n",
      "Loss: 0.00011150746286148205\n",
      "Loss: 4.3978714529657736e-05\n",
      "Loss: 3.455419937381521e-05\n",
      "Loss: 8.444111881544814e-05\n",
      "Loss: 1.0844475184512703e-07\n",
      "Loss: 7.175889913924038e-05\n",
      "Loss: 1.8062124581774697e-05\n",
      "Loss: 2.928962385340128e-05\n",
      "Loss: 4.463882578420453e-05\n",
      "Loss: 2.1836526684637647e-06\n",
      "Loss: 4.567683208733797e-05\n",
      "Loss: 5.210424205870368e-06\n",
      "Loss: 2.479075556038879e-05\n",
      "Loss: 2.0615449102479033e-05\n",
      "Loss: 5.1376800911384635e-06\n",
      "Loss: 2.6973686544806696e-05\n",
      "Loss: 5.689866497959883e-07\n",
      "Loss: 1.9371842427062802e-05\n",
      "Loss: 7.202323558885837e-06\n",
      "Loss: 7.257763627421809e-06\n",
      "Loss: 1.3612268958240747e-05\n",
      "Loss: 5.003831233807432e-07\n",
      "Loss: 1.3263391338114161e-05\n",
      "Loss: 1.225581172548118e-06\n",
      "Loss: 7.725612704234663e-06\n",
      "Loss: 5.109567609906662e-06\n",
      "Loss: 2.1601406388072064e-06\n",
      "Loss: 7.33110618966748e-06\n",
      "Loss: 1.959124418249303e-08\n",
      "Loss: 6.216621841304004e-06\n",
      "Loss: 1.0131349199582473e-06\n",
      "Loss: 3.314453351777047e-06\n",
      "Loss: 2.862001792891533e-06\n",
      "Loss: 8.614433113507403e-07\n",
      "Loss: 3.6707817798742326e-06\n",
      "Loss: 3.2761210633225346e-08\n",
      "Loss: 3.0183582566678524e-06\n",
      "Loss: 5.224966912464879e-07\n",
      "Loss: 1.643860514377593e-06\n",
      "Loss: 1.3401639762378181e-06\n",
      "Loss: 4.813375653611729e-07\n",
      "Loss: 1.7323216070508352e-06\n",
      "Loss: 1.9216289004475584e-08\n",
      "Loss: 1.497740868217079e-06\n",
      "Loss: 1.510173461838349e-07\n",
      "Loss: 9.115053103414539e-07\n",
      "Loss: 5.030698844166182e-07\n",
      "Loss: 3.509466068862821e-07\n",
      "Loss: 7.429790684909676e-07\n",
      "Loss: 4.769099248846942e-08\n",
      "Loss: 7.348101007664809e-07\n",
      "Loss: 1.4322480090811496e-08\n",
      "Loss: 5.379581011766277e-07\n",
      "Loss: 1.3234870266387588e-07\n",
      "Loss: 2.866373165488767e-07\n",
      "Loss: 2.7106338507110195e-07\n",
      "Loss: 1.0052754362277483e-07\n",
      "Loss: 3.2839355412761506e-07\n",
      "Loss: 1.1446051217944841e-08\n",
      "Loss: 2.937789247425826e-07\n",
      "Loss: 1.1999994775635514e-08\n",
      "Loss: 2.1056378329831205e-07\n",
      "Loss: 5.57474173490391e-08\n",
      "Loss: 1.1383817621890557e-07\n",
      "Loss: 1.037692243244237e-07\n",
      "Loss: 4.4087936856840315e-08\n",
      "Loss: 1.2364471047021652e-07\n",
      "Loss: 6.5658243286748075e-09\n",
      "Loss: 1.175149648702245e-07\n",
      "Loss: 8.076312951743603e-10\n",
      "Loss: 9.094171105061832e-08\n",
      "Loss: 1.293422702275393e-08\n",
      "Loss: 5.877178921309678e-08\n",
      "Loss: 2.8302020282922058e-08\n",
      "Loss: 2.985131430932597e-08\n",
      "Loss: 3.6780452461471214e-08\n",
      "Loss: 1.0093693880719457e-08\n",
      "Loss: 3.873816822874687e-08\n",
      "Loss: 1.466348042988841e-09\n",
      "Loss: 3.5454284841307526e-08\n",
      "Loss: 6.858802970022282e-10\n",
      "Loss: 2.7190253604203463e-08\n",
      "Loss: 4.082782556480424e-09\n",
      "Loss: 1.8403806478772822e-08\n",
      "Loss: 8.49201242658637e-09\n",
      "Loss: 1.0398798266919584e-08\n",
      "Loss: 1.0732037480920553e-08\n",
      "Loss: 4.267106668010001e-09\n",
      "Loss: 1.027074159054564e-08\n",
      "Loss: 7.955047176544383e-10\n",
      "Loss: 9.48105771669816e-09\n",
      "Loss: 1.1156468110451456e-11\n",
      "Loss: 8.02780686370852e-09\n",
      "Loss: 3.9775235882721915e-10\n",
      "Loss: 6.016246612006171e-09\n",
      "Loss: 1.7811544461565632e-09\n",
      "Loss: 4.4291179612798715e-09\n",
      "Loss: 3.686485117171401e-09\n",
      "Loss: 2.7061710650144732e-09\n",
      "Loss: 3.873234621920574e-09\n",
      "Loss: 8.566227172046581e-10\n",
      "Loss: 3.3697384882458437e-09\n",
      "Loss: 1.508548563844414e-10\n",
      "Loss: 2.2502111285405135e-09\n",
      "Loss: 1.882047712120638e-10\n",
      "Loss: 2.170660762246257e-09\n",
      "Loss: 6.80059508706421e-10\n",
      "Loss: 1.7108201522120225e-09\n",
      "Loss: 1.4425798333661533e-09\n",
      "Loss: 1.188406373309192e-09\n",
      "Loss: 1.3872826221117407e-09\n",
      "Loss: 4.190951474747351e-10\n",
      "Loss: 8.493467595904747e-10\n",
      "Loss: 1.3096723358585471e-11\n",
      "Loss: 5.655844126373211e-10\n",
      "Loss: 3.3663430376584813e-10\n",
      "Loss: 4.108490769816342e-10\n",
      "Loss: 4.54019766227276e-10\n",
      "Loss: 5.820766091346741e-11\n",
      "Loss: 3.570069795344466e-10\n",
      "Loss: 9.167706871426873e-11\n",
      "Loss: 3.584621766083984e-10\n",
      "Loss: 3.1917199438957766e-10\n",
      "Loss: 1.5133991282390014e-10\n",
      "Loss: 2.696954881642455e-10\n",
      "Loss: 2.2798001333979023e-11\n",
      "Loss: 2.0227161612318412e-10\n",
      "Loss: 1.799586868411751e-10\n",
      "Loss: 7.858034362095978e-11\n",
      "Loss: 1.3775812435223855e-10\n",
      "Loss: 6.402842839259293e-11\n",
      "Loss: 1.1593025844858218e-10\n",
      "Loss: 1.217510259277077e-10\n",
      "Loss: 1.6007107098148232e-11\n",
      "Loss: 1.0137834322021533e-10\n",
      "Loss: 9.167706871426873e-11\n",
      "Loss: 1.2611659980232837e-11\n",
      "Loss: 4.899144631642649e-11\n",
      "Loss: 4.899144631642649e-11\n",
      "Loss: 1.2126595734518464e-11\n",
      "Loss: 2.958889383508634e-11\n",
      "Loss: 2.958889383508634e-11\n",
      "Loss: 3.007395721343897e-11\n",
      "Loss: 8.246085238250433e-12\n",
      "Loss: 1.6007107098148232e-11\n",
      "Loss: 1.8432425724634882e-11\n",
      "Loss: 1.7462298967929613e-11\n",
      "Loss: 1.0186340486384449e-11\n",
      "Loss: 1.5036978606719487e-11\n",
      "Loss: 1.5036978606719487e-11\n",
      "Loss: 1.5036978606719487e-11\n",
      "Loss: 1.2126595734518464e-11\n",
      "Loss: 1.1641532356165829e-11\n",
      "Loss: 1.8917489102987517e-11\n",
      "Loss: 1.7462298967929613e-11\n",
      "Loss: 1.0671404732098821e-11\n",
      "Loss: 9.701277108031814e-12\n",
      "Loss: 1.5036978606719487e-11\n",
      "Loss: 1.5522043719795597e-11\n",
      "Loss: 1.5522043719795597e-11\n",
      "Loss: 1.3581787604299844e-11\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 7.761021859897799e-12\n",
      "Loss: 1.5036978606719487e-11\n",
      "Loss: 1.5036978606719487e-11\n",
      "Loss: 7.275957614183426e-12\n",
      "Loss: 7.275957614183426e-12\n",
      "Loss: 1.1641532356165829e-11\n",
      "Loss: 1.1641532356165829e-11\n",
      "Loss: 1.1641532356165829e-11\n",
      "Loss: 1.1641532356165829e-11\n",
      "Loss: 1.1641532356165829e-11\n",
      "Loss: 1.1156468110451456e-11\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.701277108031814e-12\n",
      "Loss: 1.0186340486384449e-11\n",
      "Loss: 1.0186340486384449e-11\n",
      "Loss: 9.701277108031814e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 1.0671404732098821e-11\n",
      "Loss: 1.1156468110451456e-11\n",
      "Loss: 1.1641532356165829e-11\n",
      "Loss: 1.1641532356165829e-11\n",
      "Loss: 1.1156468110451456e-11\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 7.275957614183426e-12\n",
      "Loss: 7.761021859897799e-12\n",
      "Loss: 9.701277108031814e-12\n",
      "Loss: 7.761021859897799e-12\n",
      "Loss: 7.275957614183426e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 9.216212862317441e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n",
      "Loss: 8.731149483964806e-12\n"
     ]
    }
   ],
   "source": [
    "class NumberPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential_model = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=64),\n",
    "            nn.Linear(in_features=64, out_features=64),\n",
    "            nn.Linear(in_features=64, out_features=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, training_data):\n",
    "        training_data = nn.Flatten()(training_data)\n",
    "        logits = self.sequential_model(training_data)\n",
    "        return logits\n",
    "    \n",
    "model = NumberPredictor().to(device=device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "x = torch.arange(0, 30, 1, dtype=torch.float, device=device).reshape(1, 30, 1)\n",
    "y = x + 36\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "for epoch in range(500):\n",
    "    for batch, data in enumerate(x):\n",
    "        model.train()\n",
    "        logits = model(data)\n",
    "        \n",
    "        loss = loss_fn(logits, y[batch])\n",
    "        # print(f\"loss: {loss}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        # if batch % 99 == 0:\n",
    "            # loss, current = loss.item(), batch + 1\n",
    "            # print(f\"Loss: {loss:>7f}, [{current}/{len(x)}]\")\n",
    "    print(f\"Loss: {loss}\")\n",
    "                \n",
    "# train(training_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:22.872420900Z",
     "start_time": "2024-02-05T19:03:22.424423400Z"
    }
   },
   "id": "ee95416d7a04cb27",
   "execution_count": 227
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[9035.9990]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy = torch.tensor([ [ [9000.] ] ], device=device)\n",
    "# xy *= 0.3\n",
    "model(xy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-05T19:03:31.311884800Z",
     "start_time": "2024-02-05T19:03:31.298888Z"
    }
   },
   "id": "7b547fc2143a34ca",
   "execution_count": 228
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_data = np.arange(30, dtype=int)  # x + 36\n",
    "train_label = np.arange(30, dtype=int) + 36\n",
    "shuffled_data = list(zip(train_data, train_label))\n",
    "random.shuffle(shuffled_data)\n",
    "shuffled_data = [[i for i, j in shuffled_data], [j for i, j in shuffled_data]]\n",
    "train_data = np.asarray(shuffled_data[0], dtype=int)\n",
    "train_label = np.asarray(shuffled_data[-1], dtype=int)\n",
    "# train_data = torch.from_numpy(train_data.reshape(3, 10))\n",
    "# train_label = torch.from_numpy(train_label.reshape(3, 10))\n",
    "# print(train_label)\n",
    "\n",
    "\n",
    "def train(training_data):\n",
    "    model.train()\n",
    "    # print(len(train_dataset))\n",
    "    for i, (train_x, train_y) in enumerate(training_data):\n",
    "        train_x = torch.tensor(train_x)\n",
    "        train_y = torch.tensor(train_y)\n",
    "        # Compute prediction error\n",
    "        prediction = model(train_x)\n",
    "        loss = loss_fn(prediction, train_y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, current = loss.item(), i\n",
    "        print(f\"Loss: {loss:>7f}, [{current}/{len(training_data)}]\")\n",
    "        # loss, current = loss.item(), (batch + 1) * len(images) # 64 Images\n",
    "        #     print(f\"Loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b61f2936cd463b8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
